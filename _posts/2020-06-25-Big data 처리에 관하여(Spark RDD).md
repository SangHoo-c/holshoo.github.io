---
title: Big Data 처리에 관하여(Spark RDD)
tags: RDD Spark 2020-1 BigData
---
 

## 빅-데이터처리 방법이란?

우리 삶속에는 다양한 데이터 들이 존재하고 있다. AI 관련 일을 하는 사람에게 ‘AI의 한계가 무엇인가요?’ 라고 물어보면 수치화, 즉 벡터로 나타낼 수 있는 모든 것들을 대신할 수 있을 것이라고 할 것이다. 잠깐, 왜 항상 데이터처리라는 이야기와 AI 라는 ‘컴퓨터’와 같은 주제가 함께 오는 것일까? 



![3 circles](https://dv-website.s3.amazonaws.com/uploads/2018/10/110818-pic1.png)

중국이 AI의 강자로 떠오르고 있다는 사실을 아는가? 그 이유는 공산화된 사회에 정부주도적으로 데이터 수집을 하기 때문에 그만큼 학습성능이 높은 AI의 생산이 가능하기 때문이다. 이처럼 바로 데이터 수집, 즉 처리의 목적은 AI의 성능향상, 학습에 있기 때문이다. 따라서 많은 양의 데이터, 그것도 무질서하게 나열된 dummy 데이터가 아닌, 구조화된, 최소한 일정한 모양으로 전처리된 데이터의 필요성은 갈수록 높아지고 있다. 


이제 빅데이터와 AI가 서로에게 필수불가결한 요소라는 것을 알 수 있다. 
이 리포트에서는 빅데이터 처리 방법 중에 하나인, Spark RDD를 통해서 컴퓨터가 어떻게 데이터를 처리하는지 알아보려고 한다. 


우선 구체적인 Spark에 대한 설명전에, 주된 빅데이터 처리 방법인 Map Reduce에 대하여 알아보자. 



## Map Reduce란?

대용량 데이터 처리를 위한 분산 프로그래밍 모델이다. 조금 구체적으로 표현하자면, 흩어져 있는 데이터를 수직화하여, 그 데이터를 각각의 종류 별로 모으고(Map), 필터링과 정렬을 거쳐 데이터를 뽑아내는(Reduce)하는 작업인데 이를 여러 저장공간으로부터 데이터를 한 번에 읽는 작업(분산 프로그래밍)을 통해 수행한다.  


## Hadoop? Spark? 



![spark frame](https://spark.apache.org/images/spark-stack.png)


빅데이터 처리방법의 주된 기능인 Map Reduce는 ‘하둡’과 ‘스파크’, 모두 제공한다. 먼저 개발된 기술인 하둡은 기본적으로 HDFS(Hadoop Distributed File System)이라고 불리는 분산저장시스템과 map reduce를 통해 데이터 처리를 했지만 문제는 디스크의 input/output을 사용하기 때문에 속도의 한계가 있었다. HW가격이 내려가자, 굳이 HDFS(디스크)대신 메모리를 사용하여 더 빠른 속도로, 병렬처리가 가능하며 머신러닝까지 가능한 스파크가 등장하게 되었다. 


## Spark RDD란?

Resilient, Distributed, Dataset의 약자이며, R- 데이터 손실 문제 발생 시 다시 생성할 수 있는 유실된 파티션을 재연산하여 복구하는 기능, D- 메모리를 이용하여 분산처리를 한다. D-파일을 통해 가져올 수 잇는 데이터셋 제공한다 라는 의미이다. 따라서 Spark에서 제공하는 RDD는 Spark에서 기본적인 데이터 단위라고 볼 수 있다. RDD는 두가지 작업이 가능하다. 첫째, 다양한 source 데이터를 RDD 형태로 변환시켜주는 Transformation, 둘째 RDD값을 기반으로 무엇인가를 계산하는 Action이 가능하다. 실제 코드를 잠깐 보자.


아래 코드는 data.txt에서 파일을 읽고 RDD형태로 Transformation 하는 코드이다. 

```python
lines = sc.textFile(“data.txt”)
pairs = lines.map(lamda s : (s, 1))
counts = pairs.reduceByKey( lambda a, b: a +b)
```

아래 코드는 RDD값을 기반으로 Action을 수행하는 코드이다. 


```python
counts.collect()
#RDD의 모든 데이터 리턴
counts.countByValue()
#각 값의 개수만큼 리턴 
```

이와 같은 방법으로 Spark RDD를 통해 분산 프로그래밍이 가능하다. 


## 결론

빅데이터는 AI시대를 살고 있는 우리에게 필수적인 존재이다. 따라서 처리의 목적과 과정을 알고 공부를 하는 것이 효과적이다. 다양한 방법이 존재하지만, 이 시간 Spark에 관하여 알아보았으며 이를 통해 효율적인 분산 프로그래밍이 가능함을 알게 되었으며 더 많은 양의 데이터를 인메모리 형식으로 짧은 시간안에 처린 가능하고, 내부적으로 지원하는 머신러닝 라이브러리를 통해 AI를 학습시킬 수 있음을 알게 되었다. 
